---
layout: post
title: 'A Homemade Beowulf Cluster: Part 2, Machine Configuration'
date: '2017-01-21T18:42:00.002-05:00'
author: umhau
tags:
- ssh
- CPU
- distributed computing
- DATA
- Ubuntu Server 16.04.1 LTS
- file server
- beowulf cluster
modified_time: '2017-01-31T22:49:18.137-05:00'
blogger_id: tag:blogger.com,1999:blog-2584289275272726799.post-3895699829425833161
blogger_orig_url: http://nixingaround.blogspot.com/2017/01/a-homemade-beowulf-cluster-part-2.html
---

This section starts with a set of machines all tied together with an ethernet switch and running Ubuntu Server 16.04.1. &nbsp;If the switch is plugged into the local network router, then the machines can be ssh'd into.<br /><br />This should be picking up right where Part 1 left off. <a href="https://help.ubuntu.com/community/MpichCluster" target="_blank">src</a>. &nbsp;So.<br /><h2>Enabling Scripted, Sudo Remote Access</h2>The first step in the configuration process is to modify the root-owned host files on each machine. &nbsp;I'm not doing that by hand, and I've already spent way too long trying to find a way to edit root-owned files through ssh automatically. <br /><br />It's not possible without "<a href="http://askubuntu.com/questions/16178/why-is-it-bad-to-login-as-root" target="_blank">security risks</a>". &nbsp;Since this is a local cluster, and my threat model doesn't include -- or care about -- people hacking in to the machines or me messing things up, I'm going the old fashioned way. &nbsp;I also don't care about wiping my cluster accidentally, since I'm documenting the exact process I used to achieve it (and I'm making backups of any data I create).<br /><br />Log into each machine in turn, and enter the password when prompted.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">ssh beowulf@grendel-[X]</span></span></pre>Recall that the password is<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">hrunting</span></span></pre>Create a password for the root account.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">sudo passwd root</span></span><span style="color: #333333; font-size: 14px;"><br /></span></pre><div style="text-align: justify;">At the prompt, enter your password. &nbsp;We'll assume it's the same as the previously-defined user.</div><div style="text-align: justify;"><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">hrunting</span></span></pre></div>Now the root account has a password, but it's still locked. &nbsp;Time to unlock it.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">sudo passwd -u root </span></span></pre><b>Note:</b>&nbsp;if you ever feel like locking the root account again, run this:<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">sudo passwd -l root</span></span></pre>Now you have to allow the root user to login via ssh. &nbsp;Change an option in this file:<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">sudo nano /etc/ssh/sshd_config</span></span></pre>Find the line that says:<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">PermitRootLogin prohibit-password</span></span></pre>and comment it out (so you have a record of the default configuration) and add a new line below it. They should look like this:<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">#PermitRootLogin prohibit-password<br />PermitRootLogin yes</span></span></pre>[CTRL-O] and [CTRL-X] to exit, then run:<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">sudo service ssh restart</span></span></pre>That's it! &nbsp;Now we can use <a href="https://linux.die.net/man/1/sshpass" target="_blank">sshpass</a>&nbsp;to automatically login to the machines and modify root files. &nbsp;Be careful; there is nothing between you and total destruction of your cluster.<br /><h2>Upload a custom /etc/hosts file to each machine</h2>I created a <a href="https://raw.githubusercontent.com/umhau/cluster/master/create_hosts_file.sh" target="_blank">script</a> to do this for me. &nbsp;If I could have found a simple way to set static IPs that would have been preferable, but this way I don't have to manually rebuild the file every time the cluster is restarted.<br /><b><i><br /></i></b><b><i>Note:</i></b> <i>for now, this isn't compatible with my example - it only uses node increments of digits, while my example is using letters (grendel-b vs grendel-1). &nbsp;I'll fix that later. &nbsp;For now, I'd recommend reading all the way to the end of the walkthrough before starting, and just using numbers for your node increments.</i><br /><br />Run the script from a separate computer that's on the local network (i.e., that can ssh into the machines), but which isn't one of the machines in the cluster. &nbsp;Usage of the script goes like this:<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">bash create_hosts_file.sh [MACHINE_COUNT] [PASSWORD] [HOSTNAME_BASE]</span></span></pre>Where HOSTNAME_BASE is the standard part of the hostname of each computer - if the computers were named grendel-a, grendel-b, and grendel-c, then the base would be "grendel-".<br /><br />So, continuing the example used throughout and pretending there's 5 machines in total, this is what the command would look like:<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">mkdir -p ~/scripts &amp;&amp; cd scripts<br />wget https://raw.githubusercontent.com/umhau/cluster/master/create_hosts_file.sh<br />bash create_hosts_file.sh 5 "hrunting" "grendel-"</span></span></pre>If you don't get any errors, then you're all set! You can check the files were created by ssh'ing into one of the machines and checking /etc/hosts.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">ssh beowulf@grendel-a<br />cat /etc/hosts</span></span></pre>The output should look something like this:<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">127.0.0.1     localhost<br />192.168.133.100 grendel-a<br />192.168.133.101 grendel-b<br />192.168.133.102 grendel-c<br />192.168.133.103 grendel-d</span></span></pre>If it doesn't look like that, with a line for localhost and one line after it for each machine, you're in trouble. &nbsp;Google is your friend; it worked for me. <br /><h2>Creating a Shared Folder Between Machines</h2>This way, I can put my script with fancy high-powered code in one place, and all the machines will be able to access it. <br /><br />First, dependencies. &nbsp;Install this one just on the 'master' node/computer (generally, the most powerful computer in the cluster, and definitely the one you labelled #1). <br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">sudo apt-get install nfs-server</span></span><br /></pre><div style="text-align: justify;">Next, install this on all the other machines:</div><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">sudo apt-get install nfs-client</span></span></pre>Ok, we need to define a folder that can be standardized across all the machines: same idea as having a folder labeled "Dropbox" on each computer that you want your Dropbox account synced to - except in this case, the syncing is a little different. &nbsp;Anything you put in the /mirror folder of the master node will be shared across all the other computers, but anything you put in a /mirror folder of the other nodes will be ignored. &nbsp;That's why it's called a 'mirror' - there's a single folder that's being 'mirrored' by other folders.<br /><br />We'll put it in the root directory. &nbsp;Since we're mirroring it across all the machines, call it 'mirror'. Do this on all the machines:<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">sudo mkdir /mirror</span></span></pre>Now go back to the master machine, and tell it to share the /mirror folder to the network: add a line to the /etc/exports file, and then restart the service.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">echo "/mirror *(rw,sync)" | sudo tee -a /etc/exports<br />sudo service nfs-kernel-server restart</span></span></pre>Maybe also add the following to the (rw,sync) options above:<br /><br /><ul><li><b>no_subtree_check</b>: This option prevents the subtree checking. When a shared directory is the subdirectory of a larger filesystem, nfs performs scans of every directory above it, in order to verify its permissions and details. Disabling the subtree check may increase the reliability of NFS, but reduce security.</li><li><b>no_root_squash</b>: This allows root account to connect to the folder.</li></ul><br />Great! &nbsp;Now there's a folder on the master node on the network that we can mount and automatically get stuff from. &nbsp;Time to mount it.<br /><br />There's two ways to go about this - one, we could manually mount on every reboot, or two, we could automatically mount the folder on each of the 'slave' nodes. &nbsp;I like the second option better. <br /><br />There's a file called the fstab in the /etc directory. &nbsp;It means, 'file system tabulator'. &nbsp;This is what the OS uses on startup to know which partitions to mount. &nbsp;What we're going to do is add another entry to that file - on every startup, it'll know to mount the network folder and present it like another external drive. <br /><br />On each non-master machine (i.e., all the slave machines) run this command to append a new entry to the bottom of the fstab file. &nbsp;The bit in quotes is the part getting added.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">echo "grendel-a:/mirror    /mirror    nfs" | sudo tee -a /etc/fstab</span></span></pre>That line is telling the OS a) to look for a drive located at&nbsp;grendel-a:/mirror, b) to mount it at the location /mirror, and c) that the drive is a 'network file system'. &nbsp;Remember that if you're using your own naming scheme to change 'grendel-a' to whatever the hostname of your master node is. <br /><br />Now, in lieu of rebooting the machines, run this command on each slave machine to go back through the fstab and remount everything according to whatever it (now) says. <br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">sudo mount -a</span></span></pre><h2>Establishing A Seamless Communication Protocol Between Machines</h2><h3>Create a new user</h3>This user will be used specifically for performing computations. &nbsp;If beowulf is the <i>administrative </i>user, and root is being used as the <i>setting-stuff-up-remotely-via-automated-scripts</i> user, then this is the <i>day-to-day-heavy-computations</i> user. <br /><br />The home folder for this user will be inside /mirror, and it's going to be given the same userid across all the accounts (I picked '1010') - we're making it as identical as possible for the purposes of using all the machines in the cluster as a single computational device. <br /><br />We'll call the new user 'breca'. &nbsp;Just for giggles, let's make the password 'acerb'. &nbsp;Run the first command on the master node first, and the slaves afterwards. <br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">useradd --uid 1010 -m -d /mirror/breca breca</span></span></pre>Set a password. &nbsp;Run on all nodes.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">passwd breca</span></span></pre>Add breca to the sudo group.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">sudo adduser breca sudo</span></span></pre>Since 'breca' will be handling all the files in the /mirror directory, we'll make that user the owner. <b>&nbsp;Run this only on the master node.</b><br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">sudo chown -R breca: /mirror</span></span></pre><h3>Setting up passwordless SSH for inter-node communication</h3>Next, a dependency. &nbsp;Install this to each node (master and slaves):<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">sudo aptÂ­-get install openssh-server</span></span></pre>Next, login to the new user <b>on the master node. </b><br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">su - breca</span></span></pre><b>On the master node,</b> generate an RSA key pair for the breca user. &nbsp;Keep the default location. &nbsp;If you feel like it, you can enter a 'strong' passphrase, but we've already been working under the assumption security isn't important here. &nbsp;Do what you like; nobody is going after your cluster (you hope). <br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">ssh-keygen -t rsa</span></span></pre>Add the key to your 'authorized keys'.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">cd .ssh<br />cat id_rsa.pub &gt;&gt; authorized_keys<br />cd</span></span></pre>And the nice thing is, what you've just done is being automatically mirrored to the other nodes.<br /><br />With that, you should have passwordless ssh communication between all of your nodes. &nbsp;Login to your breca account on each machine:<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">su - breca</span></span></pre>&nbsp;After logging in to your breca account on all of your machines, test your passwordless ssh capabilities by running -- say, from your master node to your first slave node --<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">ssh grendel-b</span></span></pre>or from your second slave node into your master node:<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">ssh grendel-a</span></span></pre>The only thing you should have to do is type 'yes' to confirm that some kind of fingerprint is authentic, and that's a first-time-only sort of thing. &nbsp;<b>However, because confirmation is requested, you have to perform the first login manually between each machine. &nbsp;</b>Otherwise communication could/will fail. &nbsp;I haven't checked if it's necessary to ensure communication between slave nodes, so I did those too. <br /><br />Note that since the same known_hosts file is shared among all the machines, it's only ever necessary to confirm a machine once. &nbsp;So you could just log into all the machines consecutively from the master node, and once into the master node from one of the slaves, and all the nodes would thereafter have seamless ssh communication.<br /><h4>Troubleshooting</h4>This process worked for me, following this guide exactly, so there's no reason it wouldn't work for you as well. &nbsp;If a package is changed since the time of writing, however, it may fail in the future. &nbsp;See section 7 of this guide to <a href="https://help.ubuntu.com/community/MpichCluster" target="_blank">set up a keychain</a>, which is the likely solution. <br /><br />If, after rebooting, you can no longer automatically log into your breca account within the node (master-to-slave, etc.) the /mirror mounting procedure may have been interrupted. &nbsp;i.e., possibly a network disconnect when /etc/fstab was executed such that grendel-a:/mirror couldn't be found. &nbsp;If that's the case, the machines can't connect without passwords because they don't have access to the RSA key stored in the missing /mirror/calc/.ssh directory. &nbsp;Log into each of the affected machines and remount everything in the fstab.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">sudo mount -a</span></span></pre><h3>Installing Software Tools</h3>You've been in and out of the 'beowulf' and 'breca' user accounts while setting up ssh. &nbsp;Now it's time to go back to the 'beowulf' account. &nbsp;If you're still in the breca account, run:<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">exit</span></span></pre>These are tools the cluster will need to perform computations. &nbsp;It's important to install all of this stuff prior to the MPICH2 software that ties it all together - I think the latter has to configure itself with reference to the available software.<br /><br />If you're going to be using any compilers besides GCC, this is the time to install them.<br /><br />This installs GCC. &nbsp;Run it on each computer.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">sudo apt-get install build-essential</span></span></pre>I'm probably going to want Fortran as well, so I'm including that. <br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">sudo apt-get install gfortran</span></span></pre><h3>Installing MPICH</h3><div>And now, what we've all been waiting for: the commands that will actually make these disparate machines act as a single cluster. &nbsp;Run this on each machine:</div><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">sudo apt-get install mpich</span></span></pre>You can test that the install completed successfully by running:<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">which mpiexec<br />which mpirun</span></span></pre>The output should be:<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">/usr/bin/mpiexec</span></span></pre>and<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">/usr/bin/mpirun</span></span></pre><h4>The Machinefile</h4><div>This 'machinefile' tells the mpich software what computers to use for computations, and how many processors are on each of those computers. &nbsp;The code you run on the cluster will specify how many processors it needs, and the master node (which uses the machinefile) will start at the top of the file and work downwards until it has found enough processors to fulfill the code's request. &nbsp;</div><br />First, find out how many processors you have available on each machine (the output of this command will include virtual cores). &nbsp;Run this on each machine.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">nproc</span></span></pre>Next, log back into the breca user on the master node:<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">su - breca</span></span></pre>Create a new file in the /mirror directory of the master node and open it:<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">touch machinefile &amp;&amp; nano machinefile </span></span></pre>The order of the machines in the file determines which will be accessed first. &nbsp;The format of the file lists the hostnames with the number of cores they have available.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">grendel-c:4<br />grendel-b:4<br />grendel-a:4</span></span></pre>You might want to remove one of the master node's cores for control purposes. &nbsp;Who knows? &nbsp;Up for experimentation. &nbsp;I put the master node last for a similar reason. &nbsp;The other stuff can get tied up first. <br /><br />You should be up and running! &nbsp;What follows is a short test to make sure everything is <i>actually</i>&nbsp;up and running. <br /><h3>Testing the Configuration</h3>Go to your master node, and log into the breca account.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">ssh beowulf@grendel-a<br />su - breca</span></span></pre>cd into the /mirror folder.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">cd /mirror</span></span></pre>Create a new file called mpi_hello.c<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">touch mpi_hello.c &amp;&amp; nano mpi_hello.c</span></span></pre>Put the following code into the file, and [ctrl-o] and [ctrl-x] to save and exit.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">#include &lt;stdio.h&gt;<br />#include &lt;mpi.h&gt;<br /><br />int main(int argc, char** argv) {<br />    int myrank, nprocs;<br /><br />    MPI_Init(&amp;argc, &amp;argv);<br />    MPI_Comm_size(MPI_COMM_WORLD, &amp;nprocs);<br />    MPI_Comm_rank(MPI_COMM_WORLD, &amp;myrank);<br /><br />    printf("Hello from processor %d of %d\n", myrank, nprocs);<br /><br />    MPI_Finalize();<br />    return 0;<br />}</span></span></pre>Compile the code with the custom MPI C compiler:<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">mpicc mpi_hello.c -o mpi_hello</span></span></pre>And run.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">mpiexec -n 11 -f ./machinefile ./mpi_hello</span></span></pre>Here's a breakdown of the command:<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">mpiexec              command to execute an mpi-compatible binary<br /><br />-n 11                the number of cores to ask for - this should not be<br />                     more than the sum of cores listed in the machinefile<br /><br />-f ./machinefile     the location of the machinefile<br /><br />./mpi_hello          the name of the binary to run</span></span></pre>If all went as hoped, the output should look like this:<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; overflow: auto; padding: 5px; text-align: justify; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="font-size: 14px;">Hello from processor 0 of 11<br />Hello from processor 1 of 11<br />Hello from processor 2 of 11<br />Hello from processor 3 of 11<br />Hello from processor 4 of 11<br />Hello from processor 5 of 11<br />Hello from processor 6 of 11<br />Hello from processor 7 of 11<br />Hello from processor 8 of 11<br />Hello from processor 10 of 11<br />Hello from processor 11 of 11</span></span></pre>Make sure sum of the number of processors you listed in your machinefile corresponds to the number you asked for in the mpiexec command. <br /><br />Note that you can totally ask for more processors than you actually listed - the MPICH sofware will assign multiple threads to each core to fulfill the request. &nbsp;It's not efficient, but better than errors. <br /><br />And that's it! &nbsp;You have a working, tested beowulf cluster. <br /><br />